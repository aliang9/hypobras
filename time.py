import os
import time
import random
import matplotlib.pyplot as plt
from cerebras.cloud.sdk import Cerebras

# Initialize Cerebras client
client = Cerebras(api_key=os.environ.get("CEREBRAS_API_KEY"))

def run_time_constrained_inference(prompt, model_id="llama3.1-8b", max_time=1.0):
    """
    Run inference with a time constraint.
    
    Args:
    - prompt (str): The input prompt for the model.
    - model_id (str): Model ID to use for inference.
    - max_time (float): Maximum allowed time for inference in seconds.
    
    Returns:
    - str: The response generated by the model, or a timeout message.
    - float: Time taken for inference.
    """
    start_time = time.time()
    response = ""
    
    try:
        stream = client.chat.completions.create(
            messages=[{"role": "user", "content": prompt}],
            model=model_id,
            stream=True,
        )
        
        # Collect response incrementally with a time constraint
        for chunk in stream:
            current_time = time.time()
            if current_time - start_time > max_time:
                response = "[Timeout: Inference took too long]"
                break
            content = chunk.choices[0].delta.content or ""
            response += content
        time_taken = time.time() - start_time
        return response.strip(), time_taken

    except Exception as e:
        return f"[Error: {e}]", time.time() - start_time

def evaluate_prediction(prediction, expected_answer):
    """
    Evaluate the prediction against the expected answer.
    
    Args:
    - prediction (str): The model's prediction.
    - expected_answer (str): The expected correct answer.
    
    Returns:
    - bool: Whether the prediction is considered correct.
    """
    return expected_answer in prediction.lower()

def run_time_constraint_experiment(time_limits=[0.5, 1.0, 2.0], num_trials=5):
    """
    Run the time-constrained inference experiment with various time limits.
    
    Args:
    - time_limits (list): List of maximum time limits for inference.
    - num_trials (int): Number of trials to run for each time limit.
    
    Returns:
    - dict: A dictionary mapping time limits to accuracy values.
    """
    accuracy_results = {limit: 0 for limit in time_limits}
    
    for max_time in time_limits:
        correct_predictions = 0

        for trial in range(num_trials):
            # Generate a random math problem for testing
            num1, num2 = random.randint(10, 99), random.randint(10, 99)
            prompt = f"Solve the math problem quickly: {num1} + {num2}"
            expected_answer = str(num1 + num2)

            # Run inference with the time constraint
            prediction, time_taken = run_time_constrained_inference(prompt, max_time=max_time)
            print(f"[Time Limit: {max_time}s | Trial {trial + 1}/{num_trials}]")
            print(f"Prompt: {prompt}")
            print(f"Prediction: {prediction} | Time Taken: {time_taken:.2f}s")

            # Evaluate the prediction's accuracy
            if evaluate_prediction(prediction, expected_answer):
                correct_predictions += 1

        # Calculate accuracy for the given time limit
        accuracy = correct_predictions / num_trials
        accuracy_results[max_time] = accuracy
        print(f"\nAccuracy with max_time={max_time}s: {accuracy * 100:.2f}%\n")

    return accuracy_results

def plot_time_vs_accuracy(accuracy_results):
    """
    Plot accuracy as a function of maximum inference time.
    
    Args:
    - accuracy_results (dict): Dictionary mapping time limits to accuracy values.
    """
    time_limits = list(accuracy_results.keys())
    accuracies = [accuracy_results[time] for time in time_limits]

    plt.figure(figsize=(10, 6))
    plt.plot(time_limits, accuracies, marker='o', linestyle='--', color='b')
    plt.title("Accuracy vs. Time Limit for Inference")
    plt.xlabel("Maximum Inference Time (seconds)")
    plt.ylabel("Accuracy")
    plt.ylim(0, 1)
    plt.grid()
    plt.show()

# Execute the experiment
if __name__ == "__main__":
    print("Starting time-constrained inference experiment...\n")
    results = run_time_constraint_experiment(time_limits=[0.5, 1.0, 2.0], num_trials=5)
    print("\nExperiment results:", results)
    plot_time_vs_accuracy(results)